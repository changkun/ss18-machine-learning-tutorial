{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - SS18\n",
    "\n",
    "Ludwig-Maximilians-Universität München\n",
    "* Lecturer: Prof. Dr. Volker Tresp\n",
    "* Assistant: Christian Frey, Julian Busch\n",
    "* Tutor: Changkun Ou <hi@changkun.us>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5th Tutorial - 05/17/18\n",
    "\n",
    "In this tutorial, we will create a image classifier for a 'Simpsons' character dataset. This time we will implement the classifier in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simpsons Character Classifiction in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we use \"The Simpsons Character Data\" provided by the user 'alexattia' on kaggle (source to the data: https://www.kaggle.com/alexattia/the-simpsons-characters-dataset/data) \n",
    "\n",
    "We provide a slighty preprocessed data, which will be used in the following. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Download Link</b> of the preprocessed data: \n",
    "+ Dataset: http://www.dbs.ifi.lmu.de/~frey/MLSS18/the_simpsons_char_dataset/dataset.h5\n",
    "+ Labels: http://www.dbs.ifi.lmu.de/~frey/MLSS18/the_simpsons_char_dataset/labels.h5\n",
    "\n",
    "Store the files in exactly the same folder as this notebook (Otherwise you can also adjust the paths in the following cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character Data\n",
    "First, we attach to each of the characters in the dataset a unique id which will be the class label for a specific char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_characters= {\n",
    "    0: 'abraham_grampa_simpson',\n",
    "    1: 'apu_nahasapeemapetilon',\n",
    "    2: 'bart_simpson',\n",
    "    3: 'charles_montgomery_burns',\n",
    "    4: 'chief_wiggum',\n",
    "    5: 'comic_book_guy',\n",
    "    6: 'edna_krabappel',\n",
    "    7: 'homer_simpson',\n",
    "    8: 'kent_brockman',\n",
    "    9: 'krusty_the_clown',\n",
    "    10:'lisa_simpson',\n",
    "    11:'marge_simpson',\n",
    "    12:'milhouse_van_houten',\n",
    "    13:'moe_szyslak',\n",
    "    14:'ned_flanders',\n",
    "    15:'nelson_muntz',\n",
    "    16:'principal_skinner',\n",
    "    17:'sideshow_bob'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/changkun/Desktop/week7/.env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "Like in the preceeding notebook, we will first load the data. \n",
    "In order to load the data, the library h5py has to be installed. If you haven't installed it yet, you can use the pip command:\n",
    "+ pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def load_data():\n",
    "    # in case the file is stored not in the same folder as this notebook, please adjust the path\n",
    "    h5f = h5py.File('dataset.h5','r+')\n",
    "    X_train = h5f['X_train'][:]\n",
    "    X_test = h5f['X_test'][:]\n",
    "    h5f.close()    \n",
    "\n",
    "    # in case the file is stored not in the same folder as this notebook, please adjust the path\n",
    "    h5f = h5py.File('labels.h5','r+')\n",
    "    y_train = h5f['y_train'][:]\n",
    "    y_test = h5f['y_test'][:]\n",
    "    h5f.close()  \n",
    "    \n",
    "    X_train = X_train.astype('float32') / 255.\n",
    "    X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for each layer\n",
    "\n",
    "First, we will define the hyperparameters for our network. Please note that the architecture is the same as the one in the keras solution. Keras has already some inherent methods for initializing the weight matrices. For TensorFlow, we have to define this initialization explicitly. One common initializer is the $xavier\\_initializer()$, which can be used in the following to initialize each of the weight matrices of our neural network. It automatically determines the scale of initialization based on the number of input and output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set neural network hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "wt_init = tf.contrib.layers.xavier_initializer() # weight initializer\n",
    "\n",
    "# input layer (64x64)\n",
    "# n_input = 4096\n",
    "n_input = 64*64\n",
    "\n",
    "# first convolutional layer\n",
    "n_conv_1 = 32\n",
    "k_conv_1 = 3\n",
    "\n",
    "# second convolutional layer\n",
    "n_conv_2 = 64\n",
    "k_conv_2 = 3\n",
    "\n",
    "# max pooling layer:\n",
    "pool_size = 2\n",
    "mp_layer_dropout = .25\n",
    "\n",
    "# dense layer:\n",
    "n_dense = 128\n",
    "dense_layer_dropout = .5\n",
    "\n",
    "# output layer:\n",
    "n_classes = len(map_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define placeholder Tensors for inputs and labels\n",
    "\n",
    "Next, we will **define 2 placeholders**. One for our input data (design matrix) $x$, and the other one $y$ being used for our output layer defining the output labels (=character class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of x\n",
    "x = tf.placeholder(tf.float32, [None, pic_size, pic_size, 3])\n",
    "# definition of y\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define types of layers\n",
    "In the next cell we define 3 types of layers, namely a $dense$ layer, a $conv2d$ layer and a $maxpooling2d$-layer.\n",
    "\n",
    "+ $Dense$: this layer performs a matrix multiplication of the incoming data matrix and its weight matrix. Secondly, we will add the bias to the result of the multiplication (Hint: use tf.add($\\cdot, \\cdot$) and tf.matmul($\\cdot, \\cdot$)) to perform the operations. As activation function we will use a ReLu and return the result. In total, we have $ReLU(W \\cdot x + b)$\n",
    "\n",
    "\n",
    "+ $Conv2d$: in this layer we define a convolutional layer. Note that we also need to define a $stride\\_length$ for the convolution (Hint: see tf.nn.conv2d()). After having performed the convolutional, we need to add the bias to the result. Again, we use the ReLU function as the activation function of our convolutional 2d layer.\n",
    "\n",
    "\n",
    "+ $maxpooling2d$: here we will define a maxpooling layer. For a maxpooling, we also have to define a kernel_size defining the 'window' of the pooling function. (Hint: see tf.nn.max$\\_$pool()) for further information. Here we will use $p\\_size$ for each dimension of the image (notice the $data\\_format$) \n",
    "\n",
    "The parameters are given and shall be used as given. We will define the size of the weight matrices etc. shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense layer with ReLU activation:\n",
    "def dense (x, W, b):\n",
    "    return tf.nn.relu(tf.add(tf.matmul(x, W), b))\n",
    "\n",
    "# convolutional layer with ReLU activation:\n",
    "def conv2d(x, W, b, stride_length=1):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(x, W, strides=[1, stride_length, stride_length, 1], padding='SAME'), b))\n",
    "\n",
    "# max-pooling layer:\n",
    "def maxpooling2d(x, p_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, p_size, p_size, 1], strides=[1, p_size, p_size, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design neural network architecture\n",
    "\n",
    "Now we design our neural network. Therefore, we will use a network consisting of 2 convolutional layers, 1 maxpooling layer and 2 dense layers. For the hidden layers we will use our functions defined above. We can store the weight matrices and biases in dictionaries. For now we concentrate on the architecture, next we will take care of the dictionaries and also set up the right sizes of the matrices used in our neural network.\n",
    "\n",
    "Dictionaries (assume the dictionary are already defined; the definition of the dics will be in the next cell): \n",
    "+ the weight matrices are stored in a dictionary called 'weights' (see parameter list). The keys of the dictionary are as follows: '$W\\_c1$' for the first convolutional layer, '$W\\_c2$' for the second convolutional layer, '$W\\_d1$' for the first dense layer and '$W\\_out$' for the output layer. (of course you can also use other keys if you'd like to)\n",
    "\n",
    "+ Similarily, we define a dictionary containing the biases (see parameter list). We call the dictionary 'biases'. The keys of the dictionary are as follows: '$b\\_c1$' as the bias for the first convolutional layer, '$b\\_c1$' for the bias of the second convolutional layer; '$b\\_d1$' for the bias of the first dense layer and '$b\\_out$' for the bias of the output layer.\n",
    "\n",
    "Hence, we can just attach the entries when calling our layer functions defined in the previous cells as parameters to the functions. \n",
    "\n",
    "Architecture:\n",
    "* 1: Convolutional layer with 32 neurons and a kernel_size of $3 \\times 3$. The activation function we use is a rectified linear unit (ReLU). Also, we define the input shape to be 64 x 64 x 3, as the training images' size is 64x64 with 3 colors channels (RGB)\n",
    "\n",
    "* 2: Next, we define the second convolutional layer consisting of 64 neurons and also a kernel size of 3 x 3, The activation function is again a rectified linear unit (ReLU)\n",
    "\n",
    "* 3: The next layer is a max pooling layer with a window of 2 x 2, and we use a dropout of .25.\n",
    "\n",
    "* 4: The next step is to use a dense layer. For that reason we fist flatten out the images such that they are represented as a 1d vector.\n",
    "\n",
    "* 5: We use a dense layer with 128 neurons and ReLU again as the activation function\n",
    "\n",
    "* 6: For brief of efficiency, we use also a dropout with .5\n",
    "\n",
    "* 7: The last layer is the output layer. Therefore, we use a dense layer with the number of classes as the number of neurons, i.e., $W\\cdot x + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(x, weights, biases, n_in, mp_psize, mp_dropout, dense_dropout):\n",
    "    # first convolutional layer\n",
    "    with tf.name_scope(\"conv1\"):\n",
    "        conv_1 = conv2d(x, weights['W_c1'], biases['b_c1'])\n",
    "    \n",
    "    # second convolutional layer\n",
    "    with tf.name_scope(\"conv2\"):\n",
    "        conv_2 = conv2d(conv_1, weights['W_c2'], biases['b_c2'])\n",
    "\n",
    "    # maxpool layer\n",
    "    with tf.name_scope(\"maxpool1\"):\n",
    "        pool_1 = maxpooling2d(conv_2, mp_psize)\n",
    "\n",
    "    # dropout layer\n",
    "    with tf.name_scope(\"dropout1\"):\n",
    "        dropout_1 = tf.nn.dropout(pool_1, 1 - mp_dropout)\n",
    "\n",
    "    # dense layer (first we have to flatten out the output of the previous layer)\n",
    "    with tf.name_scope(\"dense1\"):\n",
    "        flat_1 = tf.reshape(dropout_1, [-1, weights['W_d1'].get_shape().as_list()[0]])\n",
    "        dense_1 = dense(flat_1, weights['W_d1'], biases['b_d1'])\n",
    "    \n",
    "    # dropout layer\n",
    "    with tf.name_scope(\"dropout2\"):\n",
    "        dropout_2 = tf.nn.dropout(dense_1, 1 - dense_dropout)\n",
    "    \n",
    "    # output layer\n",
    "    with tf.name_scope(\"output\"):\n",
    "        return tf.add(tf.matmul(dropout_2, weights['W_out']), biases['b_out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dictionaries for storing weights and biases for each layer\n",
    "\n",
    "By now, we used the dictionaries as a black-box. We now will define them explicitly. \n",
    "\n",
    "+ The biases are defined by zero vectors with the size defined by the number of neurons for each of the layer. Again, we have the entries (keys) '$b\\_c1$', '$b\\_c2$', '$b\\_d1$', '$b\\_out$' for the dictionary containing the bias vectors.\n",
    "\n",
    "\n",
    "+ Take good care of the shape of the weight tensor. For a detailed information about the conv2d function, please refer to: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d . Let's take a more detailed view on the first convolutional layer. The expected dimension for the filter parameter for the convolutional layer is a 4 D tensor having the shape [filter_height, filter_width, in_channels, out_channels]. The filter height and width are defined by the hyperparameter $k\\_conv\\_1$. The input channels is according to the color encoding 3. And we will define the number of output channels to be the number of neurons on the second layer, i.e., $n\\_conv\\_2$. \n",
    "\n",
    "\n",
    "+ In order to compute the number of inputs to the dense layer, we have to compute the output of the maxpool-layer. Therefore, we know that the images sizes are 64x64 which are maxpooled with a pool size (p$\\_$size) of 2. Hence, in one dimension we get 64/2 = 32. The number of input neurons to the dense layer (d1) is then calculated by taking this number in each dimension of the picture 32x32. Now we know the pooled image size. This result is then multiplied by the number of neurons on the second convolutional layer: 32 x 32 x number$\\_$neurons$\\_$secondConvLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of dict for biases\n",
    "bias_dict = {\n",
    "    'b_c1': tf.Variable(tf.zeros([n_conv_1])),\n",
    "    'b_c2': tf.Variable(tf.zeros([n_conv_2])),\n",
    "    'b_d1': tf.Variable(tf.zeros([n_dense])),\n",
    "    'b_out': tf.Variable(tf.zeros([n_classes]))\n",
    "}\n",
    "\n",
    "# calculate number of inputs to dense layer:\n",
    "full_square_length = np.sqrt(n_input)\n",
    "pooled_square_length = int(full_square_length / pool_size)\n",
    "dense_inputs = pooled_square_length ** 2 * n_conv_2\n",
    "\n",
    "# definition of dict for weights\n",
    "weight_dict = {\n",
    "    'W_c1': tf.get_variable('W_c1', [k_conv_1, k_conv_1, 3, n_conv_1], initializer=wt_init),\n",
    "    'W_c2': tf.get_variable('W_c2', [k_conv_2, k_conv_2, n_conv_1, n_conv_2], initializer=wt_init),\n",
    "    'W_d1': tf.get_variable('W_d1', [dense_inputs, n_dense], initializer=wt_init), \n",
    "    'W_out': tf.get_variable('W_out', [n_dense, n_classes],  initializer=wt_init)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model\n",
    "Now, we are ready to build the model by calling the net() function from above with the parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition for our predictions\n",
    "predictions = net(x, weight_dict, bias_dict, n_input,\n",
    "                     pool_size, mp_layer_dropout, dense_layer_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model's loss and its optimizer\n",
    "Also, we define our cost function where we use again the softmax cross entropy (Hint: tf.nn.softmax_cross_entropy_with_logits($\\cdot, \\cdot$)).\n",
    "As optimizer we will use the ADAM method in order to minimize our cost function. (Hint: tf.train.AdamOptimizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the cost function\n",
    "cost = tf.reduce_mean (tf.nn.softmax_cross_entropy_with_logits_v2(logits=predictions, labels=y))\n",
    "\n",
    "# defintion of the optimizer\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define evaluation metrics\n",
    "Next, we also define some evaluation metrics. What we want to have is the percentage of correct predictions made by our neural network. Therefore, we can use the $tf.equal(\\cdot, \\cdot)$ in order to check if the arguments by our true class labels (y) and the predictions made by our network are the same. In order to get a value in percentage we can use the $tf.reduce\\_mean(\\cdot)$ function. Note that the reduce function expects numeric values (hint: tf.cast(.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defintion of accuracy (in percentage)\n",
    "correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(y,1))\n",
    "acc_pct = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create operation for variable initialization\n",
    "Like we have already seen in the introduction to TensorFlow, we also need to define a global initializer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of initializer\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure history log\n",
    "To create a logging of the performance of our neural network we can use the tf.summary operations. We will just have a short view on how to use TensorBoard here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_percentage:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar(\"cost\", cost)\n",
    "tf.summary.scalar(\"accuracy_percentage\", acc_pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network in a session\n",
    "As, we use batches for the training, we also have to explicitly define a $next\\_batch()$ function. As, we want to concentrate on TensorFlow and its functionality, we provide one solution on how to define such a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN!\n",
    "\n",
    "Finally, we are all set up to train our neural network in a TensorFlow session. The steps more explicitly:\n",
    "+ first we define a session\n",
    "+ next we will run our global initializer for all the variables in our network\n",
    "+ next we create a iteration for the number of epochs.\n",
    "+ in each epoch we iterate for the mini-batches; in more detail: we will get the next_batch() and we will use this batch as the input for our network (feed the batch in the neural network). We will run the optimizer, the calculation of the cost function and the computation of the accuracy in percentage.\n",
    "+ for a more detailed output, we can also aggregate the cost and the accuracy in percentage and print it on the console (c.f. verbose in keras)\n",
    "+ test the model according to the cost function and accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: cost = 2.739 , acc = 15.96 %\n",
      "Epoch 002: cost = 2.121 , acc = 34.64 %\n",
      "Epoch 003: cost = 1.712 , acc = 46.38 %\n",
      "Epoch 004: cost = 1.422 , acc = 54.50 %\n",
      "Epoch 005: cost = 1.256 , acc = 59.22 %\n",
      "Training Complete.\n",
      "Test Model...\n",
      "Test Cost: 1.557\n",
      "Test Accuracy: 51.64 %\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    # run initializer\n",
    "    session.run(init_op)\n",
    "\n",
    "    # Define FileWriter for logs\n",
    "    train_writer = tf.summary.FileWriter( './logs/1/train ', session.graph)\n",
    "    \n",
    "    # iterate epochs and batches and run statement for TensorFlow calculations\n",
    "    # use  merge = tf.summary.merge_all() to get values for tf.summaries defined aboove\n",
    "    # and attach it to param list in session.run() to actually retrieve the results\n",
    "    # with train_writer.add_summary(summary, cnt) we can write for each batch within \n",
    "    # an epoch the results to the log files. Note that the counter cnt is an indicator\n",
    "    # for the log-id\n",
    "    cnt = 0\n",
    "    for epoch in range(5):\n",
    "        avg_cost = avg_acc_pct = 0.0        \n",
    "        n_batches = int(len(X_train) / batch_size)\n",
    "        for i in range(n_batches):\n",
    "            batch_x , batch_y = next_batch(batch_size, X_train, y_train)\n",
    "            \n",
    "            cnt+=1\n",
    "            merge = tf.summary.merge_all()\n",
    "            \n",
    "            summary, _ , batch_cost, batch_acc = session.run([merge, optimizer, cost, acc_pct],\n",
    "                                               feed_dict={x:batch_x, y:batch_y})\n",
    "\n",
    "            train_writer.add_summary(summary, cnt)\n",
    "            \n",
    "            # aggregate cost and acc for each batch in epoch\n",
    "            avg_cost += batch_cost / n_batches\n",
    "            avg_acc_pct += batch_acc / n_batches\n",
    "            \n",
    "        \n",
    "        # verbose\n",
    "        print (\"Epoch {:03}: cost = {:.3f} , acc = {:.2f} %\".format(\n",
    "            epoch+1, avg_cost, avg_acc_pct))\n",
    "\n",
    "    print(\"Training Complete.\")\n",
    "    \n",
    "    # Test model within session\n",
    "    print (\"Test Model...\")\n",
    "    test_cost = cost.eval({x: X_test, y: y_test})\n",
    "    test_accuracy_pct = acc_pct.eval({x: X_test, y: y_test})\n",
    "    \n",
    "    print(\"Test Cost: {:.3f}\".format(test_cost))\n",
    "    print(\"Test Accuracy: {:.2f} %\".format(test_accuracy_pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short introduction and to show the Interface of TensorBoard, please refer to the tutorials\n",
    "# End of this Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
